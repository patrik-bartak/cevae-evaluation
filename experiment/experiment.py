from compare import *
from typing import *
from scipy.stats import multivariate_normal, beta
from datetime import datetime
import pyro.distributions as dist
import random
import torch

from utils import get_complex_latent_and_proxy, get_complex_second_latent_and_proxy


class Experiment:
    """
    Creates an experiment builder which can be used to create some specific experiments.
    """

    def __init__(self, seed: int = None, name: str = None):
        self.name = name
        self.reset(seed, name)

    def __hash__(self):
        if self.name is not None:
            return self.name
        return super().__hash__()

    def reset(self, seed: int = None, name: str = ""):
        """
        Resets the experiment.
        :param name: name
        :param seed: seed for random state.
        :return: self
        """
        self.results: List[pd.DataFrame] = []
        self.generators: List[(Generator, int)] = []
        self.models: List[CausalMethod] = []
        self.metrics: Dict[str, Callable[[List[float], List[float]], float]] = {}
        if seed is not None:
            np.random.seed(seed)
        self.seed = seed
        self._set_defaults()
        self.trained: bool = False
        self.count: int = 0
        seed = f"seeded_{seed}" if seed is not None else f"randomized"
        datetime_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        hash = f"{self.__hash__()}"
        self.directory = f'experiments/experiment_{name}_{seed}_{datetime_str}_{hash}'
        os.makedirs(self.directory, exist_ok=True)
        return self

    def clear(self):
        """
        Completely clear generated data, but keep the models
        :return: self
        """
        self.results = []
        self.generators = []
        self._set_defaults()
        self.trained = False
        return self

    def get_result(self):
        return self.results[0]['eATE'].iat[0]

    def add_custom_generator(self, generator: Generator, sample_size: int = 500):
        """
        Adds a custom generator
        :param generator: Generator to be added
        :param sample_size: Number of samples to be generated by the generator
        :return: self
        """
        self.generators.append((generator, sample_size))
        return self

    def add_custom_model(self, model: CausalMethod):
        """
        Add a causal model into the experiment
        :param model: Model to be added
        :return: self
        """
        self.models.append(model)
        return self

    def add_custom_metric(self, name: str, scoring_function: Callable[[List[float]], float]):
        """
        Add a custom metric to the experiment
        :param name: Name of the metric
        :param scoring_function: Lambda function that takes in an array of results and outputs a number
        :return: self
        """
        self.metrics[name] = scoring_function
        return self

    # RUN EXPERIMENT

    def run(self, save_data: bool = True, save_graphs: bool = True, show_graphs: bool = False):
        """
        Runs the experiment. First trains all the models and then evaluates them
        :param save_data: Boolean whether the generated data should be stored
        :param save_graphs: Boolean whether graphs should be stored
        :param show_graphs: Boolean whether graphs should be shown
        :return: self
        """
        model_dictionary = {}
        for model in self.models:
            model_dictionary[str(model)] = model
        metric_dictionary = self.metrics
        results = np.array([0 for _ in metric_dictionary])
        for generator, sample_size in self.generators:
            generator.directory = self.directory + "/" + generator.directory[5:]
            generator.generate_data(sample_size, save_data=save_data, save_graphs=save_graphs, show_graphs=show_graphs)
            result = run(model_dictionary, metric_dictionary,
                         data_file=generator.directory + generator.generated_files['data'][-1],
                         samples=sample_size, save_table=save_data,
                         dir=generator.directory, show_graphs=show_graphs, save_graphs=save_graphs).to_numpy()
            results = results + result

        results = results / len(self.generators)
        final_results = []
        for index, result in enumerate(results):
            result = list(result)
            result.insert(0, list(model_dictionary.keys())[index])
            final_results.append(result)
        columns = list(metric_dictionary.keys())
        columns.insert(0, 'method_name')
        final_result = pd.DataFrame(final_results, columns=columns)
        final_result = final_result.set_index('method_name')
        self.results.append(final_result)
        save_pandas_table(self.directory + '/final_table', final_result)
        self.trained = True
        return final_result

    def test_specific_set(self, test_set=pd.DataFrame, truth_set=pd.DataFrame):
        """
        Test all the trained models on specific data set provided by the user. Make sure to first run the 'run' method.
        :param test_set: Dataframe of feature vectors to be tested
        :param truth_set: Dataframe of corresponding expected results
        :return: self
        """
        assert self.trained, "Models are not trained yet. Please make sure you run the full experiment first!"
        self.count += 1
        columns = [name for name in self.metrics]
        columns.insert(0, 'method_name')
        df = pd.DataFrame([], columns=columns)
        for model in self.models:
            predictions = model.estimate_causal_effect(test_set)
            row = [model.__str__()]
            for metric in self.metrics.values():
                score = metric(truth_set.to_numpy(), predictions)
                row.append(score)
            df.loc[len(df.index)] = row
        df = df.set_index('method_name')
        self.results.append(df)
        save_pandas_table(self.directory + f'/table_comparing_specific_value_{self.count}', df)
        return self

    # MODELS

    def add_dummy_model(self):
        """
        Adds dummy model for testing
        :return: New experiment with dummy model
        """
        return self.add_custom_model(DummyMethod())

    def add_causal_forest(self, number_of_trees=100, min_leaf_size=10, honest: bool=True):
        """
        Adds Causal Forest model to the experiment
        :param number_of_trees: Number of tress in the model
        :param min_leaf_size: Minimum leaf size
        :param honest: Honesty property on or off
        :return: New experiment with Causal Forest model
        """
        return self.add_custom_model(CausalForest(number_of_trees, k=min_leaf_size, honest=honest, id=len(self.models)))

    def add_dragonnet(self, dimensions):
        """
        Adds Dragonnet model to the experiment
        :param dimensions: then number of data dimensions
        :return: New experiment with Dragonnet model
        """
        return self.add_custom_model(DragonNet(dimensions, id=len(self.models)))

    def add_cevae(self,
                  dimensions,
                  outcome_dist="bernoulli",
                  model_dist="normal",
                  latent_dim=20,
                  hidden_dim=200,
                  num_layers=3,
                  num_samples=100,
                  batch_size=100,
                  num_epochs=100,
                  learning_rate=1e-3,
                  learning_rate_decay=0.1,
                  weight_decay=1e-4,
                  ):
        """
        Adds CEVAE to the experiment
        :param dimensions: Number of data dimensions
        :param outcome_dist: The modelled outcome distribution
        :param model_dist: The latent distribution to model
        :param latent_dim: The number of latent dimensions
        :param hidden_dim: The number of hidden dimensions
        :param num_layers: The number of layers
        :param num_samples: Number of samples from the latent
        :param batch_size: Batch size
        :param num_epochs: Num epochs to train for
        :param learning_rate: Learning rate
        :param learning_rate_decay: Learning rate decay
        :param weight_decay: Weight decay
        :return: New experiment with CEVAE model
        """
        return self.add_custom_model(
            CausalEffectVariationalAutoencoder(
                dimensions,
                outcome_dist,
                model_dist,
                latent_dim,
                hidden_dim,
                num_layers,
                num_samples,
                batch_size,
                num_epochs,
                learning_rate,
                learning_rate_decay,
                weight_decay,
                id=len(self.models)
            )
        )

    # METRICS

    def add_all_metrics(self):
        """
        Add all metrics to the experiment
        :return: New experiment with all metrics
        """
        return self.add_ate_error() \
            .add_ate_percent_error() \
            .add_true_ate() \
            .add_estimated_ate() \
            .add_pehe_rmse() \
            .add_pehe_mse() \
            .add_pehe_mae()

    def add_true_ate(self):
        """
        Add true ATE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_custom_metric('True ATE',
                                      lambda ite_truth, ite_pred: np.mean(ite_truth))

    def add_estimated_ate(self):
        """
        Add estimated ATE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_custom_metric('Est. ATE',
                                      lambda ite_truth, ite_pred: np.mean(ite_pred))

    def add_mean_squared_error(self):
        """
        Add MSE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_custom_metric('PEHE (MSE)',
                                      lambda ite_truth, ite_pred: np.sum(
                                          [(ite_truth[i] - ite_pred[i]) ** 2 for i in range(len(ite_truth))]) / np.prod(ite_truth.shape))

    def add_pehe_mse(self):
        """
        Add MSE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_mean_squared_error()

    def add_pehe_rmse(self):
        """
        Add RMSE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_root_mean_squared_error()

    def add_root_mean_squared_error(self):
        """
        Add RMSE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_custom_metric('PEHE (RMSE)',
                                      lambda ite_truth, ite_pred: np.sqrt(
                                          np.sum(
                                              [(ite_truth[i] - ite_pred[i]) ** 2 for i in range(len(ite_truth))]
                                          ) / np.prod(ite_truth.shape)
                                      ))

    def add_absolute_error(self):
        """
        Add MAE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_custom_metric('PEHE (MAE)',
                                      lambda ite_truth, ite_pred: np.sum(
                                          [abs(ite_truth[i] - ite_pred[i]) for i in range(len(ite_truth))]) / np.prod(ite_truth.shape))

    def add_pehe_mae(self):
        """
        Add MAE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_absolute_error()

    def add_ate_error(self):
        """
        Add eATE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_custom_metric('eATE',
                                      lambda ite_truth, ite_pred: np.abs(np.mean(ite_truth) - np.mean(ite_pred)))

    def add_ate_percent_error(self):
        """
        Add %eATE metric to the experiment
        :return: New experiment with the added metric
        """
        return self.add_custom_metric('eATE (%)',
                                      lambda ite_truth, ite_pred: np.abs((np.mean(ite_truth) - np.mean(ite_pred)) / np.mean(ite_truth)) * 100)

    # DATA GENERATORS

    def _set_defaults(self):
        """
        Sets the default functions used throughout the project.
        :return:
        """
        self.main_effect = lambda x: 2 * x[0] - 1
        self.treatment_effect = lambda x: (1 + 1 / (1 + np.exp(-20 * (x[0] - 1 / 3)))) * (
                1 + 1 / (1 + np.exp(-20 * (x[1] - 1 / 3))))
        # https://en.wikipedia.org/wiki/Beta_distribution
        self.treatment_propensity = lambda x: (1 + beta.pdf(x[0], 2, 4)) / 4
        self.noise = lambda: 0.05 * np.random.normal(0, 1)
        self.treatment_function = lambda propensity, noise: 1 if np.random.random() <= propensity else 0
        self.outcome_function = lambda main, treat, treat_eff, noise: main + (treat - 0.5) * treat_eff + noise
        # E[Y1 - Y0 | X] = 0.5 * treat_eff(x) + 0.5*treat_eff(x) = treat_eff(x)
        self.cate = lambda x: self.treatment_effect(x)

    def add_cevae_generated_data(self, distributions, proxy_function,
                                 treatment_function, outcome_function,
                                 dimensions, sample_size: int = 500, name: str=None):
        """
        Adds data generator object based on data generating functions
        :param dimensions: Number of data dimensions
        :param treatment_function: Function on t
        :param outcome_function: Function on y
        :param proxy_function: Function on X
        :param distributions: Distributions of Z
        :param sample_size: Number of data rows
        :param name: Dataset name
        :return: New experiment with added dataset
        """
        if distributions is None:
            distributions = [np.random.random]
        if proxy_function is None:
            proxy_function = lambda features: [[feat] for feat in features]
        generator = data_generator.CevaeGenerator(distributions, proxy_function,
                                                  treatment_function, outcome_function,
                                                  dimensions, name)
        return self.add_custom_generator(generator, sample_size=sample_size)

    def add_complex_latent_normal_generator(self, dimensions, sample_size, args):
        data_seed = args.data_seed
        # Normal - Age
        # Inverse exponential - Income
        # Uniform - Day of the week
        torch.manual_seed(data_seed)
        random.seed(data_seed)
        np.random.seed(data_seed)
        # distributions = [lambda: (dist.Bernoulli(0.5).sample().cpu().item() * 2) - 1]
        # distributions = [lambda: dist.Uniform(0, 2).sample().cpu().item()]
        distributions, proxy_function = get_complex_latent_and_proxy(dimensions, sample_size)
        # noise = np.random.uniform()
        # treatment_function = lambda z: dist.Bernoulli(logits=z[0]).sample().cpu().item()
        if dimensions == 1:
            print("Using 1 latent variable")
            treatment_function = lambda z: dist.Bernoulli(logits=z[0] + z[0] + z[0]).sample().item()
            def outcome_function(z, t, mean=False):
                # out_mean = z[0] * 0.7 + 0.3 * z[0] + (t - 0.5) * (z[0] + z[0]) / 2
                out_mean = z[0] * 0.7 + 0.3 * z[0] + 0.5 * (t - 0.5) * (z[0] + z[0]) / 2
                if mean:
                    return dist.Normal(out_mean, 0.1).mean.item()
                else:
                    return dist.Normal(out_mean, 0.1).sample().item()
        elif dimensions == 3:
            print("Using 3 latent variables")
            treatment_function = lambda z: dist.Bernoulli(logits=z[0] + z[1] + z[2]).expand([sample_size, 1]).sample()
            def outcome_function(z, t, mean=False):
                # out_mean = z[2] * 0.7 + 0.3 * z[0] + (t - 0.5) * (z[0] + z[1]) / 2
                out_mean = z[2] * 0.7 + 0.3 * z[0] + 0.5 * (t - 0.5) * (z[0] + z[1]) / 2
                if mean:
                    return dist.Normal(out_mean, 0.1).expand([sample_size, 1]).mean
                else:
                    return dist.Normal(out_mean, 0.1).expand([sample_size, 1]).sample()
        else:
            raise ValueError(f"Invalid number of dimensions for the latent space: {dimensions}")
        if dimensions == 0:
            dimensions = 3
        return self.add_cevae_generated_data(distributions, proxy_function, treatment_function, outcome_function,
                                             dimensions, sample_size=sample_size, name='complex_1_generator')

    def add_complex_second_latent_normal_generator(self, dimensions, sample_size, args):
        data_seed = args.data_seed
        # Normal - Age
        # Inverse exponential - Income
        # Uniform - Day of the week
        torch.manual_seed(data_seed)
        random.seed(data_seed)
        np.random.seed(data_seed)
        # distributions = [lambda: (dist.Bernoulli(0.5).sample().cpu().item() * 2) - 1]
        # distributions = [lambda: dist.Uniform(0, 2).sample().cpu().item()]
        distributions, proxy_function = get_complex_second_latent_and_proxy(dimensions)
        # noise = np.random.uniform()
        # treatment_function = lambda z: dist.Bernoulli(logits=z[0]).sample().cpu().item()
        if dimensions == 1:
            print("Using 1 latent variable")
            treatment_function = lambda z: dist.Bernoulli(logits=z[0] + z[0] + z[0]).sample().cpu().item()
            def outcome_function(z, t, mean=False):
                out_mean = z[0] * 0.7 + 0.3 * z[0] + (t - 0.5) * (z[0] + z[0]) / 2
                if mean:
                    return dist.Normal(out_mean, 0.1).mean.cpu().item()
                else:
                    return dist.Normal(out_mean, 0.1).sample().cpu().item()
        elif dimensions == 3:
            print("Using 3 latent variables")
            treatment_function = lambda z: dist.Bernoulli(logits=z[0] + z[1] + z[2]).sample().cpu().item()
            def outcome_function(z, t, mean=False):
                out_mean = z[2] * 0.7 + 0.3 * z[0] + (t - 0.5) * (z[0] + z[1]) / 2
                if mean:
                    return dist.Normal(out_mean, 0.1).mean.cpu().item()
                else:
                    return dist.Normal(out_mean, 0.1).sample().cpu().item()
        elif dimensions == 0:
            print("Using 0 latent variables")
            treatment_function = lambda z: dist.Bernoulli(logits=z[0] + z[1] + z[2]).sample().cpu().item()
            def outcome_function(z, t, mean=False):
                out_mean = z[2] * 0.7 + 0.3 * z[0] + (t - 0.5) * (z[0] + z[1]) / 2
                if mean:
                    return dist.Normal(out_mean, 0.1).mean.cpu().item()
                else:
                    return dist.Normal(out_mean, 0.1).sample().cpu().item()
        else:
            raise ValueError(f"Invalid number of dimensions for the latent space: {dimensions}")
        if dimensions == 0:
            dimensions = 3
        return self.add_cevae_generated_data(distributions, proxy_function, treatment_function, outcome_function,
                                             dimensions, sample_size=sample_size, name='complex_1_generator')
